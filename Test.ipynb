{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sanaz/anaconda3/lib/python3.6/site-packages/statsmodels/compat/pandas.py:56: FutureWarning: The pandas.core.datetools module is deprecated and will be removed in a future version. Please use the pandas.tseries module instead.\n",
      "  from pandas.core import datetools\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "import seaborn as sb\n",
    "\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "#import sklearn\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sanaz/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2717: DtypeWarning: Columns (22,32,34,49,55) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n",
      "/home/sanaz/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2717: DtypeWarning: Columns (49) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "train_2016 = pd.read_csv('train_2016_v2.csv')\n",
    "properties_2016 = pd.read_csv('properties_2016.csv')\n",
    "train_2017 = pd.read_csv('train_2017.csv')\n",
    "properties_2017 = pd.read_csv('properties_2017.csv')\n",
    "# print(\"train2016:\", train_2016.shape)\n",
    "# print(\"train2017:\", train_2017.shape)\n",
    "# print(\"prop2016:\", properties_2016.shape)\n",
    "# print(\"prop2017:\", properties_2016.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(81275, 60)\n",
      "(70613, 60)\n",
      "(81275, 60)\n",
      "(70613, 60)\n"
     ]
    }
   ],
   "source": [
    "# Preparing the test and train datasets\n",
    "\n",
    "#prop16_train16 = pd.merge(train_2016,properties_2016, on='parcelid', how='left')\n",
    "#prop16_train17 = pd.merge(train_2017,properties_2016, on='parcelid', how='left')\n",
    "#prop17_train16 = pd.merge(train_2016,properties_2017, on='parcelid', how='left')\n",
    "#prop17_train17 = pd.merge(train_2017,properties_2017, on='parcelid', how='left')\n",
    "# print(prop16_train16.shape)\n",
    "# print(prop16_train17.shape)\n",
    "# print(prop17_train16.shape)\n",
    "# print(prop17_train17.shape)\n",
    "# print('********************')\n",
    "\n",
    "# taking 10% of each dataset for test\n",
    "\n",
    "test1 = prop16_train16.iloc[0:9000]\n",
    "test2 = prop16_train17.iloc[7000:14000]\n",
    "test3 = prop17_train16.iloc[18000:27000]\n",
    "test4 = prop17_train17.iloc[21000:28000]\n",
    "\n",
    "# print(test1.shape)\n",
    "# print(test2.shape)\n",
    "# print(test3.shape)\n",
    "# print(test4.shape)\n",
    "# print('********************')\n",
    "\n",
    "# taking 90% of each data set for train\n",
    "\n",
    "train1 = prop16_train16.drop(prop16_train16.index[0:9000])\n",
    "train2 = prop16_train17.drop(prop16_train17.index[7000:14000])\n",
    "train3 = prop17_train16.drop(prop17_train16.index[18000:27000])\n",
    "train4 = prop17_train17.drop(prop17_train17.index[21000:28000])\n",
    "print(train1.shape)\n",
    "print(train2.shape)\n",
    "print(train3.shape)\n",
    "print(train4.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA CLEANSING AND ANALYSIS\n",
    "\n",
    "# Remove ID columns\n",
    "train1 = train1.drop(['parcelid', 'logerror', 'transactiondate', 'propertyzoningdesc', \n",
    "                         'propertycountylandusecode'], axis=1)\n",
    "train2 = train2.drop(['parcelid', 'logerror', 'transactiondate', 'propertyzoningdesc', \n",
    "                          'propertycountylandusecode'], axis=1)\n",
    "train3 = train3.drop(['parcelid', 'logerror', 'transactiondate', 'propertyzoningdesc', \n",
    "                          'propertycountylandusecode'], axis=1)\n",
    "train4 = train4.drop(['parcelid', 'logerror', 'transactiondate', 'propertyzoningdesc', \n",
    "                          'propertycountylandusecode'], axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA CLEANSING AND ANALYSIS\n",
    "\n",
    "# Converting value of Objects from Y/N to true/false\n",
    "for c in train1.dtypes[train1.dtypes == object].index.values:\n",
    "    train1[c] = (train1[c] == True)\n",
    "for c in train2.dtypes[train2.dtypes == object].index.values:\n",
    "    train2[c] = (train2[c] == True)\n",
    "for c in train3.dtypes[train3.dtypes == object].index.values:\n",
    "    train3[c] = (train3[c] == True)\n",
    "for c in train4.dtypes[train4.dtypes == object].index.values:\n",
    "    train4[c] = (train4[c] == True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA CLEANSING AND ANALYSIS\n",
    "\n",
    "# Looking for nulls\n",
    "# print(\"Num of nulls in train1 : \",train1.isnull().sum())\n",
    "#print(\"Num of nulls in train2 : \",train2.isnull().sum())\n",
    "# print(\"Num of nulls in train3 : \",train3.isnull().sum())\n",
    "# print(\"Num of nulls in train4 : \",train4.isnull().sum())\n",
    "\n",
    "train1 = train1.fillna(train1.median())\n",
    "train2 = train2.fillna(train2.median())\n",
    "train3 = train3.fillna(train3.median())\n",
    "train4 = train4.fillna(train4.median())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Stability Selection via Randomized Lasso\n",
    "\n",
    "#In a nutshell, this method serves to apply the feature selection on different parts of the data \n",
    "#and features repeatedly until the results can be aggregated. Therefore stronger features \n",
    "#(defined as being selected as important) will have greater scores in this method as compared to weaker features\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
